{"cells":[{"cell_type":"markdown","id":"931A6dn_uuSy","metadata":{"id":"931A6dn_uuSy"},"source":["# Machine Learning Application: Sustainable Biofuels\n","\n","---\n","\n","### Goals of the notebook\n","\n","* Train a machine learning model to predict the flash point of fuels using their spectra\n","\n","* Apply the train test split technique to validate our machine learning model\n","\n","* Use feature selection as a way to avoid the machine learning model from overfitting\n","\n","\n","---\n","\n","### Table of Contents\n","\n","* [Import spectra and flash point data + EDA](#section1)<br>\n","\n","* [EXERCISE 1 - Normalization](#section2)<br>\n","\n","* [Train Test Split](#section3)<br>\n","\n","  * [Overfitting](#subsection1)<br>\n","\n","* [EXERCISE 2 - Feature Selection](#section4)<br>\n","\n","  * [Task A](#subsection2)<br>\n","\n","  * [Task B](#subsection3)<br>\n","\n","* [EXERCISE 3 - Model Training](#section5)<br>\n","\n","  * [Linear Regression](#subsection4)<br>\n","\n","  * [Random Forest Regressor](#subsection5)<br>\n","\n","* [EXERCISE 4 - Challenge](#section6)<br>\n","\n","\n","---"]},{"cell_type":"markdown","id":"AlKe_3A2WPhh","metadata":{"id":"AlKe_3A2WPhh"},"source":["Currently, aviation accounts for a big part of Greenhouse Gas (GHG) emissions, and switching to sustainable aviation fuel could halve carbon emissions by 2050.\n","\n","However, the development of sustainable aviation fuels is limited by significant technical barriers. Specifically, because property testing for novel fuels is expensive and requires a large volume of the sample, it is performed pretty late in the development cycle. This leads to the scale-up of blends that are ultimately not viable to be used as fuels or underperform.\n","\n","The purpose of this exercise is to train a machine learning model capable of predicting fuel properties early in the development cycle. This can be used as a first pass check before scale up of promising blends, helping derisk and accelerate alternative jet fuel research and development.\n"]},{"attachments":{},"cell_type":"markdown","id":"nVRtUfb3xkpH","metadata":{"id":"nVRtUfb3xkpH"},"source":["\n","Let's first get started by importing any libraries that are needed."]},{"cell_type":"code","execution_count":null,"id":"Kt6jBpBdx3H6","metadata":{"id":"Kt6jBpBdx3H6"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","id":"0f0a5968","metadata":{"id":"0f0a5968"},"source":["# Import spectra and flash point data + EDA <a name = \"section1\">"]},{"cell_type":"markdown","id":"dcab6599","metadata":{"id":"dcab6599"},"source":["Using pandas, let's import the 'flash_point_and_spectra_data_short.csv' database from the data folder. Then we will perform some EDA to get to know the dataset."]},{"cell_type":"code","execution_count":null,"id":"327dd55a","metadata":{"id":"327dd55a"},"outputs":[],"source":["df=pd...."]},{"cell_type":"code","execution_count":null,"id":"IvKtIw8UvWDc","metadata":{"id":"IvKtIw8UvWDc"},"outputs":[],"source":["#Exploratory Data Analysis 1\n","\n","#This can be head, tail, columns, describe, etc.\n","\n","df..."]},{"cell_type":"code","execution_count":null,"id":"nQuY9oYAFpqS","metadata":{"id":"nQuY9oYAFpqS"},"outputs":[],"source":["#Exploratory Data Analysis 2\n","\n","..."]},{"cell_type":"code","execution_count":null,"id":"KxXTz2BwFu3u","metadata":{"id":"KxXTz2BwFu3u"},"outputs":[],"source":["#Exploratory Data Analysis 3\n","\n","..."]},{"cell_type":"markdown","id":"bce86d38","metadata":{"id":"bce86d38"},"source":["This database has the columns 'fuel', and 'flash point', which give you the flash point and names of different fuels.  \n","\n","The other columns, represent the FTIR spectra of these fuels. FTIR spectra is like the fingerprint of a molecule or blends of molecules, it gives us insights into the structure of the sample. The data is taken at the frequencies indicated by the column names of the database.\n"]},{"cell_type":"markdown","id":"49c8d719","metadata":{"id":"49c8d719"},"source":["### Tasks:\n","\n","In the cells below, find out:\n","\n","- How many samples does this database have? *Hint: What function that we have used before that lets us find the number of rows of a DataFrame?*\n","- How many spectra values do we have for each sample?"]},{"cell_type":"code","execution_count":null,"id":"8644bff5","metadata":{"id":"8644bff5"},"outputs":[],"source":["# EXERCISE - How many samples does the database have?\n","\n","..."]},{"cell_type":"code","execution_count":null,"id":"6iFT2hnRG0uh","metadata":{"id":"6iFT2hnRG0uh"},"outputs":[],"source":["# EXAMPLE - How many spectra values do we have for each sample?\n","\n","# We could use a for loop like this:\n","spectra_columns=[]\n","for column in df.columns:\n","  if ('flash point' not in column) and ('fuel' not in column):\n","    spectra_columns.append(column)\n","len(spectra_columns)\n","\n","# or we could use list comprehensions.\n","# The syntax for a list comprehension is: [expression(element) for element in old_list if condition]\n","# For example, the for loop above could be replaced by:\n","len([c for c in df.columns if 'flash point' not in c and 'fuel' not in c])"]},{"cell_type":"markdown","id":"bnHYHP-CVopO","metadata":{"id":"bnHYHP-CVopO"},"source":["Let's see what an FTIR spectra looks like.\n","\n","Plot the spectra of the first fuel:\n","- the x-axis will be the columns of the database, excluding the 'flash point' and 'fuel' columns. These are the frequencies the spectra data is taken at.\n","- the y-axis will be the values of those columns in the first row (again, excluding the 'fuel' and 'flash point' columns). This is the absorbance of a specific sample and the frequencies in the x-axis.\n"]},{"cell_type":"code","execution_count":null,"id":"eb79d4ed","metadata":{"id":"eb79d4ed"},"outputs":[],"source":["x_data = [pd.to_numeric(c) for c in df.columns if 'fuel' not in c and 'flash point' not in c]\n","y_data = df.loc[0,[c for c in df.columns if 'fuel' not in c and 'flash point' not in c]]\n","\n","# Go back to your previous notebook 08 data visualization to remind yourself how to use matplotlib to create axes labels and create a line plot.\n","\n","plt.title('...')\n","plt.xlabel('...')\n","plt.ylabel('...')\n","plt.plot(x_data, y_data)"]},{"cell_type":"markdown","id":"BQO-JKlSGbGr","metadata":{"id":"BQO-JKlSGbGr"},"source":["We will be using the spectra curves to predict the flash point of these fuels."]},{"cell_type":"markdown","id":"884444cd","metadata":{"id":"884444cd"},"source":["# EXERCISE 1- Normalization <a name = \"section2\">\n","\n","Due to intrumental error and noise, the raw spectra are not always comparable between each other. For example, taking the spectra of the same sample in two different rooms with different moisture levels can result in different spectra.\n","\n","To help mitigate some of this error, we will be normalizing the spectra.\n","\n","Normalizing data means that we adjust the scale of our data by comparing our numbers on a relative scale instead of directly comparing them. This helps with data that takes on a large range of numbers, and it eliminates the issue of comparing categories that are on different scales.\n"]},{"cell_type":"markdown","id":"OiMI6Zw_JsEh","metadata":{"id":"OiMI6Zw_JsEh"},"source":["### Tasks:\n","\n","* 1a. Replace any negative values with 0.\n","\n","*For Task 1a, look up (on the internet) the .clip() function from pandas.*\n","\n","* 1b. Normalize the spectra of each sample so that the maximum value for each spectra is 1."]},{"cell_type":"code","execution_count":null,"id":"3846ec80","metadata":{"id":"3846ec80"},"outputs":[],"source":["# EXERCISE 1a.\n","all_spectra_data=df[[c for c in df.columns if 'fuel' not in c and 'flash point' not in c]]\n","\n","# Use the pandas function .clip() to clip any negative values from all_spectra_data\n","all_spectra_data_clipped=all_spectra_data.clip(lower=...)\n"]},{"cell_type":"code","execution_count":null,"id":"0ghiGz_xJ9ip","metadata":{"id":"0ghiGz_xJ9ip"},"outputs":[],"source":["# EXERCISE 1b.\n","#The following line gives you the maximum value for each row of spectra\n","maximum_spectra_value_for_each_row= all_spectra_data_clipped.max(axis=1)\n","\n","#Replace '...' in the following line to divide all_spectra_data by the maximum spectra value for each row\n","all_spectra_data_clipped_and_normalized=all_spectra_data_clipped.div(...,axis=0) #rename variable"]},{"cell_type":"markdown","id":"8mg8b_cy4eyX","metadata":{"id":"8mg8b_cy4eyX"},"source":["# Train Test Split <a name=\"section3\">"]},{"cell_type":"markdown","id":"6EletdDfKMF9","metadata":{"id":"6EletdDfKMF9"},"source":["We will use the spectra **(features)** to predict the flash point **(target variable)** of these fuels.\n","\n","## Splitting into train and testing\n","Although we want to use all our data to build a model, it is important to also get a sense of how the model will be able to predict new data. Because we can't magically generate new data, we will be setting a portion of our data aside (called the **test set**) to test our models. Usually, 20% to 30% of the data is used as a test set.\n","\n","The rest of the data is called a **train set**, and we will be using it to train our models. After training a model, we see how well the model predicts the property in the test set.\n","\n","### Task: split data into training and testing with the help of sklearn's train_test_split\n","You will reserve 30% of the data for testing, set a random seed of 42 to ensure reproducibility. All \"ensure reproducibility\" means in this case is that you will get the same train test split of your data every time you run the cell. For example, if I didn't have an argument defining my random state, then if I have to rerun my notebook or rerun the cell, I would get different data in my training vs test set which I don't want to happen. I want the data in my training and test set to be consistent throughout my notebook.\n","\n","The number 42 has no inherent significance (unless you have read The Hitchiker's Guide to the Galaxy), but is often used as a random seed for simplicity.\n","\n","\n","## Each variable is explained below:\n","* X_train: The spectra of fuels in the train set\n","* X_test: The spectra of fuels in the test set\n","* y_train: The flash point of fuels in the train set\n","* y_test: The flash point of fuels in the test set"]},{"cell_type":"code","execution_count":null,"id":"e638247f","metadata":{"id":"e638247f"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","\n","X_train,X_test,y_train,y_test=train_test_split(all_spectra_data_clipped_and_normalized,\n","                                              df['flash point'],test_size=0.3,random_state=42)"]},{"cell_type":"markdown","id":"aX0rFiJeKz8Q","metadata":{"id":"aX0rFiJeKz8Q"},"source":["## Overfitting<a name=\"subsection1\">"]},{"cell_type":"markdown","id":"a56a74bf","metadata":{"id":"a56a74bf"},"source":["When we have too many features, choosing which ones to include or exclude is one of the most important parts of training a model. We could use all the spectra values to train a model that would fit the training data perfectly, but this will result in overfitting.\n","\n","Overfitting happens when the model follows the training data too closely, and picks up any variability or noise from the data. In this case, the model will be able to predict the training set really well but will fail to predict new data accurately.\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1CYtxRiUkks0_4ZI32MG2ZG_kGg-CXeyc\" width=\"500\" height=\"200\" />\n","\n","To help illustrate this, consider the following example:\n","- Imagine a student is studying for a math test by practicing with a set of problems.\n","- Instead of learning the general method to solve a problem, they memorize the answers to each specific question.\n","- The day of the test, the problems are similar but not exactly the same as the practice problems.\n","- Because they memorized the specific answers, rather than understanding how to solve the problems, they struggle to answer the questions on the test.\n","\n","When this happens in machine learning, we may get a model that is only able to accurately predict samples in the training set. That model would not be useful.\n","\n","\n","To help avoid overfitting, we must cut down on the number of features and only select the ones that will help us predict the flash point accurately, this process is called **feature selection**."]},{"cell_type":"markdown","id":"HaNrPu0LLt6K","metadata":{"id":"HaNrPu0LLt6K"},"source":["\n","### Task:\n","Fit a model with all variables to your training set.\n","\n","Similar to the Cell Phone Design Challenge, we will need to specify a machine learning model to help us with **predicting** flash point of fuels using spectra.\n","\n","We will again be using the `scikit-learn` library which implements a variety of different machine learning models and other analysis tools. As we are dealing with **regression** in machine learning, we will specifically be using the **Random Forest Regressor** model. (You can learn more about random forest models from the [scikit-learn user guide](https://scikit-learn.org/stable/modules/ensemble.html#forest))."]},{"cell_type":"code","execution_count":null,"id":"b4f7d646","metadata":{"id":"b4f7d646"},"outputs":[],"source":["#Note: this may take a while\n","\n","from sklearn.ensemble import RandomForestRegressor\n","sample_model=RandomForestRegressor(random_state=42)\n","sample_model.fit(X_train,y_train)\n"]},{"cell_type":"markdown","id":"XdcAIQZzNjHb","metadata":{"id":"XdcAIQZzNjHb"},"source":["There are different ways of calculating the performance of a model. In the Cell Phone Design Challenge, you used the root mean squared error (RMSE). For this exercise, you will use a different error metric called mean absolute error (MAE). There are a couple of differences between these two metrics:\n","- RMSE gives more weight to larger errors (because of the squaring step), while MAE is not affected by them as much.\n","- MAE is easier to interpret, it is simply the average error for all the data.\n","\n","Both are useful to measure accuracy, but highlight different aspects of the errors in our predictions.\n","\n","To calculate the mean absolute error, you will be using the scikit-learn's `mean_absolute_error()` function."]},{"cell_type":"code","execution_count":null,"id":"p8O5iZGjNERy","metadata":{"id":"p8O5iZGjNERy"},"outputs":[],"source":["from sklearn.metrics import mean_absolute_error\n","\n","print('Mean Absolute Error: '+str(mean_absolute_error(y_test,sample_model.predict(X_test))))"]},{"cell_type":"markdown","id":"4Ium2uF55r8X","metadata":{"id":"4Ium2uF55r8X"},"source":["**We will be comparing this MAE to that of a model trained after feature selection.**"]},{"cell_type":"markdown","id":"68c3c4eb","metadata":{"id":"68c3c4eb"},"source":["# Exercise 2: Feature Selection <a name = \"section4\">"]},{"cell_type":"markdown","id":"8B0qOj0cOHST","metadata":{"id":"8B0qOj0cOHST"},"source":["Recall that we use the process of **feature selection** to avoid overfitting. We must must cut down on the number of features (in our case spectra) and only select the ones that will help us predict the flash point accurately."]},{"cell_type":"markdown","id":"Df8Vjm7mwvA3","metadata":{"id":"Df8Vjm7mwvA3"},"source":["## Task A: <a name = \"subsection2\">\n","\n","Some of the wavelengths have very low values for most our samples, these could represent functional groups (an atom or group of atoms within a molecule that has similar chemical properties) and chemical bonds that are not present in any fuel.\n","\n","In the cell below, delete the columns that have a value of less than 0.1 for at least 50% of all samples."]},{"cell_type":"code","execution_count":null,"id":"4f83fedf","metadata":{"id":"4f83fedf"},"outputs":[],"source":["# Task A - fill in the '...'\n","\n","# Use the variance_threshold function\n","from biofuel_helper_functions import variance_threshold\n","\n","columns_less_than_threshold=variance_threshold(X_train,minimum_value=...,sample_percentage=...)\n","\n","\n","X_train=X_train.drop(columns_less_than_threshold,axis=1)\n","X_test=X_test.drop(columns_less_than_threshold,axis=1)\n"]},{"cell_type":"markdown","id":"nYRz2Z3ZiP8h","metadata":{"id":"nYRz2Z3ZiP8h"},"source":["Let's find the mean absolute error again. How does the accuracy change? Should we keep these columns? Should we delete more?"]},{"cell_type":"code","execution_count":null,"id":"1dd8270f","metadata":{"id":"1dd8270f"},"outputs":[],"source":["# Fill in the '...' (see how mean_absolute_error was used a couple cells above)\n","\n","threshold_model=RandomForestRegressor(random_state=42)\n","threshold_model.fit(X_train,y_train)\n","print('Mean Absolute Error: '+str(mean_absolute_error(...,threshold_model.predict(...))))"]},{"cell_type":"markdown","id":"y2VjLWT_Za9z","metadata":{"id":"y2VjLWT_Za9z"},"source":["## Task B: <a name = \"subsection3\">\n","We will be introducing two types of feature selection: binning and recursive feature elimination"]},{"cell_type":"markdown","id":"740d0892","metadata":{"id":"740d0892"},"source":["### Task B1\n","- The binning function will aggregate multiple columns together. For example, if we had values at 2500,2501,2502,2503,2504,and 2505 cm-1 before, we could bin all these values into 1 column by averaging them. It's a way of smoothing the spectra of these fuels to eliminate tiny peaks that may not be important. The parameters to change are the \"width\" of the window we are smoothing by.\n","\n","\n","For example, in the image below, the window width is 1, and we are averaging 3 values into the same bin.\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1MmAVqwWPxeP6RBLT1ovLsuVIcC_ZIpJu\" width=\"900\" height=\"450\" />\n"]},{"cell_type":"markdown","id":"KfYnSwvOd_o0","metadata":{"id":"KfYnSwvOd_o0"},"source":["\n","### Task B2\n","- Recursive feature elimination is a type of feature selection method that searches for a subset of features by removing the least important ones. It works by fitting a model, ranking the features based on importance, and removing the X least important ones. Then, it fits another model with the remaining features and repeats this process until there are Y features left. The ranking of these features is the reverse order in which they were eliminated, i.e.: the ones left at the very end are ranked 1 and seen as most important, while the first ones to be eliminated are ranked last and see as the least important features. There are different parameters to change here, including how many features to delete at each step (the X), and when to stop the process (when we have reached Y features).\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1y2zfLA0SYP-ml37cea_4Tt2HKy7E84ka\" width=\"1000\" height=\"600\" />"]},{"cell_type":"markdown","id":"Beis8aNPM6YY","metadata":{"id":"Beis8aNPM6YY"},"source":["## Goal: Choose a feature selection method and reduce the number of features.\n"]},{"cell_type":"markdown","id":"jcWhtlEUevXY","metadata":{"id":"jcWhtlEUevXY"},"source":["Use the bin_spectra_features function to bin spectra with a window width of 40, then see the error on the test set."]},{"cell_type":"code","execution_count":null,"id":"f433bcce","metadata":{"id":"f433bcce"},"outputs":[],"source":["# Task B1\n","\n","from biofuel_helper_functions import bin_spectra_features\n","\n","bin_model=RandomForestRegressor(random_state=42)\n","bin_X_train=bin_spectra_features(X_train,window_width=...)\n","bin_X_test=bin_spectra_features(...,window_width=...)\n","bin_model.fit(bin_X_train,y_train)\n","print('Mean Absolute Error: '+str(mean_absolute_error(y_test,bin_model.predict(...))))\n"]},{"cell_type":"markdown","id":"xCPz17Q8NMZN","metadata":{"id":"xCPz17Q8NMZN"},"source":["Now use recursive feature elimination with a step size of 100 to select the 50 most important features. Your n_features_to_select is when you want to stop the process of recursive feature elimination (in this case when you have the 50 most important features). Your step is how many features you want to eliminate at each \"elimination round.\""]},{"cell_type":"code","execution_count":null,"id":"ed00cd67","metadata":{"id":"ed00cd67"},"outputs":[],"source":["# Task B2\n","#RFE: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html\n","## Use RandomForestRegressor as the estimator\n","## This process may take a while depending on step size and final number of features chosen\n","## trying n_features_to_select=50 and step=20 will take about a minute to run, any step size smaller will take longer\n","\n","from sklearn.feature_selection import RFE\n","import time\n","tic=time.time()\n","rfe=RFE(estimator=RandomForestRegressor(random_state=42),n_features_to_select=...,step=...)\n","\n","\n","\n","rfe.fit(X_train,y_train)\n","rfe_X_train=rfe.transform(X_train)\n","rfe_X_test=rfe.transform(X_test)\n","rfe_model=RandomForestRegressor(random_state=42)\n","rfe_model.fit(rfe_X_train,y_train)\n","toc=time.time()\n","\n","print('Mean Absolute Error: '+str(mean_absolute_error(... , ...)))"]},{"cell_type":"markdown","id":"5e22bf6d","metadata":{"id":"5e22bf6d"},"source":["Let's plot what our predictions with recursive feature elimination look like to get a visual sense of model performance."]},{"cell_type":"code","execution_count":null,"id":"i3dpxzda1gin","metadata":{"id":"i3dpxzda1gin"},"outputs":[],"source":["train_predictions=rfe_model.predict(rfe_X_train)\n","test_predictions=rfe_model.predict(rfe_X_test)\n","\n","plt.plot(np.arange(260,310,1),np.arange(260,310,1),c='grey')\n","plt.scatter(train_predictions,y_train,label='Train set')\n","plt.scatter(test_predictions,y_test,label='Test set')\n","plt.legend()"]},{"cell_type":"markdown","id":"8da438c0","metadata":{"id":"8da438c0"},"source":["If you have time, train multiple models with different step sizes, the plot the accuracy.\n"]},{"cell_type":"code","execution_count":null,"id":"e-ix21_ajt27","metadata":{"id":"e-ix21_ajt27"},"outputs":[],"source":["# try a step size of 75,50,and 25 and plot their accuracy against the step size 0f 100\n","\n","#first, initialize the RFE models\n","rfe_step75=RFE(estimator=RandomForestRegressor(random_state=42),n_features_to_select=50,step=...)\n","rfe_step50=RFE(estimator=RandomForestRegressor(random_state=42),n_features_to_select=50,step=...)\n","rfe_step25=RFE(estimator=RandomForestRegressor(random_state=42),n_features_to_select=50,step=...)\n","\n","\n","\n","\n","#then fit the RFE models\n","rfe_step75.fit(X_train,y_train)\n","rfe_step50.fit(X_train,y_train)\n","rfe_step25.fit(X_train,y_train)\n","\n","#transform the train and test sets\n","rfe_X_train75=rfe_step75.transform(X_train)\n","rfe_X_test75=rfe_step75.transform(X_test)\n","rfe_X_train50=rfe_step50.transform(X_train)\n","rfe_X_test50=rfe_step50.transform(X_test)\n","rfe_X_train25=rfe_step25.transform(X_train)\n","rfe_X_test25=rfe_step25.transform(X_test)\n","\n","\n","#train random forest models\n","rfe_model_step75=RandomForestRegressor(random_state=42)\n","rfe_model_step75.fit(rfe_X_train75,y_train)\n","rfe_model_step50=RandomForestRegressor(random_state=42)\n","rfe_model_step50.fit(rfe_X_train50,y_train)\n","rfe_model_step25=RandomForestRegressor(random_state=42)\n","rfe_model_step25.fit(rfe_X_train25,y_train)\n","\n","#get the train and test predictions for each model\n","train_predictions_75=rfe_model_step75.predict(rfe_X_train75)\n","train_predictions_50=rfe_model_step50.predict(rfe_X_train50)\n","train_predictions_25=rfe_model_step25.predict(rfe_X_train25)\n","test_predictions_75=rfe_model_step75.predict(rfe_X_test75)\n","test_predictions_50=rfe_model_step50.predict(rfe_X_test50)\n","test_predictions_25=rfe_model_step25.predict(rfe_X_test25)\n","\n","#plot the train and test mean absolute error\n","plt.scatter([100,75,50,25],[mean_absolute_error(train_preds,y_train) for train_preds in [train_predictions,train_predictions_75,train_predictions_50,train_predictions_25]],c='tab:blue',s=10,label='train mean absolute error')\n","plt.plot([100,75,50,25],[mean_absolute_error(train_preds,y_train) for train_preds in [train_predictions,train_predictions_75,train_predictions_50,train_predictions_25]],c='tab:blue')\n","\n","plt.scatter([100,75,50,25],[mean_absolute_error(test_preds,y_test) for test_preds in [test_predictions,test_predictions_75,test_predictions_50,test_predictions_25]],c='tab:orange',s=10,label='test mean absolute error')\n","plt.plot([100,75,50,25],[mean_absolute_error(test_preds,y_test) for test_preds in [test_predictions,test_predictions_75,test_predictions_50,test_predictions_25]],c='tab:orange')\n","\n","plt.xlabel('RFE step size')\n","plt.ylabel('Mean Absolute error')\n","plt.legend()\n","\n"]},{"cell_type":"markdown","id":"71b5c17d","metadata":{"id":"71b5c17d"},"source":["# Exercise 3: Model Training <a name = \"section5\">\n","We will be testing the features we found with two different models: **Linear Regression** and a **Random Forest Regressor**. We will walk through the steps through Linear Regression and you will get to try coding the Random Forest Regressor!\n","\n","## Task:\n","Train both models and see how well it predicts both the training set and the test set. We will use the following to evaluate our models:\n","\n","- Mean absolute error\n","\n","- Parity plot showing predicted flash point vs. experimental flash point for all the data"]},{"cell_type":"markdown","id":"ftH3C7UQqiwb","metadata":{"id":"ftH3C7UQqiwb"},"source":["## Linear Regression <a name = \"subsection4\">"]},{"cell_type":"markdown","id":"aVBk7SuCqubB","metadata":{"id":"aVBk7SuCqubB"},"source":["First, **specify the model**:"]},{"cell_type":"code","execution_count":null,"id":"DwOgAduVq99V","metadata":{"id":"DwOgAduVq99V"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","\n","linreg=LinearRegression()"]},{"cell_type":"markdown","id":"LHpF5voprAsO","metadata":{"id":"LHpF5voprAsO"},"source":["Now we will **train the model**.\n","\n","We will use `rfe_X_train` as our input features (spectra) and `y_train` as our target variable (flash point)."]},{"cell_type":"code","execution_count":null,"id":"JWoc_PTTrj5y","metadata":{"id":"JWoc_PTTrj5y"},"outputs":[],"source":["linreg.fit(rfe_X_train,y_train)"]},{"cell_type":"markdown","id":"ywyU2qs8rnKe","metadata":{"id":"ywyU2qs8rnKe"},"source":["Let's **evaluate our model** by finding the mean absolute error and creating a parity plot."]},{"cell_type":"code","execution_count":null,"id":"a3f19a4d","metadata":{"id":"a3f19a4d"},"outputs":[],"source":["print('Mean Absolute Error for Linear Regression: '+str(mean_absolute_error(y_test,linreg.predict(rfe_X_test))))\n","\n","train_predictions=linreg.predict(rfe_X_train)\n","test_predictions=linreg.predict(rfe_X_test)\n","plt.plot(np.arange(260,310,1),np.arange(260,310,1),c='grey')\n","plt.scatter(...,y_train,label='Train set')\n","plt.scatter(...,y_test,label='Test set')\n","plt.legend()"]},{"cell_type":"markdown","id":"AZPWrvP9r1Kt","metadata":{"id":"AZPWrvP9r1Kt"},"source":["## Random Forest Regressor <a name = \"subsection5\">"]},{"cell_type":"markdown","id":"HT5oPIPwsBw9","metadata":{"id":"HT5oPIPwsBw9"},"source":["Now your turn! First let's **specify the model**.\n","\n","Make sure to use the argument `random_state=42`\n","\n","*Hint: Look up the documentation on [Random Forest Regressor models on sci-kit learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) to help you.*"]},{"cell_type":"code","execution_count":null,"id":"FJXPYQgHsA96","metadata":{"id":"FJXPYQgHsA96"},"outputs":[],"source":["from ... import ...\n","rf=...\n"]},{"cell_type":"markdown","id":"8ad_TxURtRAb","metadata":{"id":"8ad_TxURtRAb"},"source":["Now we will **train the model**.\n","\n","Similar to Linear Regression, we will use `rfe_X_train` as our input features (spectra) and `y_train` as our target variable (flash point)."]},{"cell_type":"code","execution_count":null,"id":"3OeIQKW_tXDS","metadata":{"id":"3OeIQKW_tXDS"},"outputs":[],"source":["rf.fit(..., ...)\n"]},{"cell_type":"markdown","id":"W3pgfoYWtbs-","metadata":{"id":"W3pgfoYWtbs-"},"source":["Let's **evaluate our model** by finding the mean absolute error and creating a parity plot."]},{"cell_type":"code","execution_count":null,"id":"qg0POPsWrw6T","metadata":{"id":"qg0POPsWrw6T"},"outputs":[],"source":["print('Mean Absolute Error for RandomForest: '+str(mean_absolute_error(... , ...)))\n","\n","train_predictions=rf.predict(...)\n","test_predictions=rf.predict(...)\n","plt.plot(np.arange(260,310,1),np.arange(260,310,1),c='grey')\n","plt.scatter(...,y_train,label='Train set')\n","plt.scatter(...,y_test,label='Test set')\n","plt.legend()"]},{"cell_type":"markdown","id":"pUO4TZrStgDr","metadata":{"id":"pUO4TZrStgDr"},"source":["## DISCUSSION\n","\n","Which model worked better? Why? Support your reasoning with evidence from evaluating the models."]},{"cell_type":"markdown","id":"7e07b8cc","metadata":{"id":"7e07b8cc"},"source":["# Exercise 4 - Challenge <a name = \"section6\">\n","Your friend just made this fuel that they're convinced is the perfect fuel. Your challenge is to predict the flash point for this new fuel with your model.\n","\n","Do not forget to do the same transformations (normalizing, deleting the same features) as with your training!\n","\n","Note that the new database does not have the 'fuel' and 'flash point' columns."]},{"cell_type":"code","execution_count":null,"id":"f89f4df3","metadata":{"id":"f89f4df3"},"outputs":[],"source":["new_spectra=pd.read_csv('data/jet_a_spectra_short.csv')\n","\n","new_spectra.head()\n"]},{"cell_type":"markdown","id":"d_CXIB8BAwFZ","metadata":{"id":"d_CXIB8BAwFZ"},"source":["First, clip the negative values"]},{"cell_type":"code","execution_count":null,"id":"EFddcUAguVcy","metadata":{"id":"EFddcUAguVcy"},"outputs":[],"source":["#clip negative values\n","new_spectra=new_spectra.clip(lower=...)\n"]},{"cell_type":"markdown","id":"Mgaw7G_NAy2w","metadata":{"id":"Mgaw7G_NAy2w"},"source":["Then, normalize the data"]},{"cell_type":"code","execution_count":null,"id":"C5WIAnhjucDL","metadata":{"id":"C5WIAnhjucDL"},"outputs":[],"source":["#normalize the new data\n","maximum_spectra_value_for_new_spectra=new_spectra.'...'\n","new_spectra=new__spectra.div(...)\n"]},{"cell_type":"code","execution_count":null,"id":"egLC5vlnsj48","metadata":{"id":"egLC5vlnsj48"},"outputs":[],"source":["# delete the columns that didn't pass the variance threshold from before with the .drop() function.\n","# for this, we will simply keep the columns that were kept in X_train\n","columns_less_than_threshold=[c for c in new_spectra.columns if c not in X_train.columns]\n","new_spectra=new_spectra.drop(...,axis=1)\n"]},{"cell_type":"markdown","id":"HnM8SpBaA1ip","metadata":{"id":"HnM8SpBaA1ip"},"source":["We will now pick a feature selection method and model that we like.\n","You will need to transform the new data the same way that you did the data from yesterday.\n","\n","For example, if you want to use a model that was trained on binned data, don't forget to also bin the new data."]},{"cell_type":"code","execution_count":null,"id":"n5FnuwWQ_FhY","metadata":{"id":"n5FnuwWQ_FhY"},"outputs":[],"source":["## pick the feature selection method of your choice, you could bin the data, use recursive feature elimination, or both."]},{"cell_type":"code","execution_count":null,"id":"q0aNK9V5ut-V","metadata":{"id":"q0aNK9V5ut-V"},"outputs":[],"source":["#if you want to use binning:\n","from ... import ...\n","new_spectra_binned=bin_spectra_features(...,window_width=...)\n","bin_X_train=bin_spectra_features(...,window_width=...)\n","bin_X_test=bin_spectra_features(...,window_width=...)\n","\n","\n","# if you want to use recursive feature elimination\n","from ... import ...\n","rfe=RFE(estimator=RandomForestRegressor(random_state=42),n_features_to_select=...,step=...)\n","rfe.fit(... , ...)\n","rfe_X_train=rfe.transform(...)\n","rfe_X_test=rfe.transform(...)\n","new_spectra_rfe=rfe.transform(...)"]},{"cell_type":"markdown","id":"AZ3H7mVRFzNu","metadata":{"id":"AZ3H7mVRFzNu"},"source":["Now pick a model, train it to the data from yesterday (pick the training set using the same feature selection method as the one you chose above), and predict the flash point of the new fuel"]},{"cell_type":"code","execution_count":null,"id":"HO1saVhjFp0S","metadata":{"id":"HO1saVhjFp0S"},"outputs":[],"source":["#import the model of your choice\n","from ... import ...\n","model= ...\n","\n","### Here are your choices for the above: ###\n","## Random Forest Regressor\n","#from sklearn.ensemble import RandomForestRegressor\n","#model = RandomForestRegressor(random_state=42)\n","# OR\n","## Linear Regression\n","#from sklearn.linear_model import LinearRegression\n","#model=LinearRegression()\n","\n","\n","# fit the model to the appropriate train set (either binned or with RFE)\n","model.fit(... , ...)\n","\n","\n","# then predict the new spectra.\n","# if you trained your model with binned train data, use the binned new spectra from the cell above\n","# if you trained your model with recursive feature elimination as a feature selection method, use the new spectra that was also transformed with recursive feature elimination\n","new_prediction=model.predict(...)\n","\n","print(new_prediction)\n"]},{"cell_type":"markdown","id":"vS288-dbuF0d","metadata":{"id":"vS288-dbuF0d"},"source":["They were able to measure the flash point for this fuel at 318.15K, how close is your model to the actual value? \\\n","Let's plot the predictions for the test set and the new fuel below."]},{"cell_type":"code","execution_count":null,"id":"bW1aRpqsuqsx","metadata":{"id":"bW1aRpqsuqsx"},"outputs":[],"source":["# fill in the '...' to match the version of the train and test set (binned or RFE) that you used above\n","train_predictions= model.predict(...)\n","test_predictions=model.predict(...)\n","\n","\n","plt.plot(np.arange(260,310,1),np.arange(260,310,1),c='grey')\n","plt.scatter(train_predictions,y_train,label='Train set')\n","plt.scatter(test_predictions,y_test,label='Test set')\n","plt.scatter(new_prediction,315.15,label='New fuel')\n","plt.legend()"]},{"cell_type":"markdown","id":"_Hzg5wlTYATp","metadata":{"id":"_Hzg5wlTYATp"},"source":["Repeat Exercises 2 and 3 in this notebook above, changing different parameters for your model and feature selection, did the accuracy increase?"]},{"cell_type":"markdown","id":"1407fb41","metadata":{"id":"1407fb41"},"source":["# Summary:\n","\n","What worked best for the combination of feature selection and model? What went wrong? What can we do differently to increase the accuracy of our model?"]},{"cell_type":"markdown","id":"hwY1Y2z9v-u7","metadata":{"id":"hwY1Y2z9v-u7"},"source":["Notebook developed by: Ana Comesana"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"},"vscode":{"interpreter":{"hash":"1de2c9b99b67c74942ae62ba9f4273989cfcd06b191aeb61677ac908252bed7d"}}},"nbformat":4,"nbformat_minor":5}
