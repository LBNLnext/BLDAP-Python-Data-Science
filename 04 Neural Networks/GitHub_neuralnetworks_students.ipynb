{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1787f370",
      "metadata": {
        "id": "1787f370"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL;\n",
        "\n",
        "###DO NOT MODIFY###\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rcParams['figure.dpi'] = 300\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "447fcb10-bcaf-42ea-8925-9382a313994c",
      "metadata": {
        "id": "447fcb10-bcaf-42ea-8925-9382a313994c"
      },
      "source": [
        "## Neural networks <a name='section6'></a>\n",
        "Our **brain** is an extremely complex system which is made of many **neurons** and many **connections** between them.  \n",
        "\n",
        "A single neuron is a much simpler thing, but because there's so many of them with so many connections, the brain overall becomes a complex entity.\n",
        "\n",
        "Artificial neural networks take inspiration from the brain.  \n",
        "They are made of many simple building blocks (the neurons) that are interconnected.\n",
        "The connections can have different strenghts, called **weights**.\n",
        "By putting togher many many many neurons and connections, the overall network becomes complex.\n",
        "\n",
        "A single neuron is a linear function + an additional easy step called **activation**.  \n",
        "The network is a complex (nonlinar) function.\n",
        "\n",
        "\n",
        "------\n",
        "📌 Neurons   \n",
        "📌 Connections  \n",
        "📌 Weights  \n",
        "📌 Activation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0RsUWb-kKrmD",
      "metadata": {
        "id": "0RsUWb-kKrmD"
      },
      "source": [
        "# Exercise 1: A simple neural network\n",
        "\n",
        "Create your first neural network!  \n",
        "Let's assume it has two **input** neurons, $x_1$, $x_2$ and one **output** neuron $y$.  \n",
        "\n",
        "* Define $x_1$ as a number of your choice\n",
        "* Define $x_2$ as another number of your choice\n",
        "* Define the **weight** of the connection from input 1 to the output, $w_1$, as a number of your choice\n",
        "* Do the same for $w_2$\n",
        "* Define a **bias** of your choice $b$\n",
        "* Compute the value of the output neuron as $y = w_1 \\times x_1 + w_2 \\times x_2 + b $\n",
        "* Apply the **activation function**:\n",
        "  * if $y$ is positive, do nothing (active neuron)\n",
        "  * otherwise change the value of $y$ to zero (\"sleepy\" neuron)\n",
        "\n",
        "\n",
        "**Question**  \n",
        "What do you obtain with these two sets of values? Is the output neuron on or off? If it's on, what's its value?\n",
        "* $x_1 = 1, \\quad x_2 = 2, \\quad w_1 = -1.3, \\quad w_2 = 2.5, \\quad b = 0.1$\n",
        "* $x_1 = -0.1, \\quad x_2 = 0.3, \\quad w_1 = 0.2, \\quad w_2 = -1.5, \\quad b = 0.45$\n",
        "\n",
        "------\n",
        "📌 Input  \n",
        "📌 Output  \n",
        "📌 Weight  \n",
        "📌 Bias  \n",
        "📌 Activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UFA_tqdBfl_i",
      "metadata": {
        "id": "UFA_tqdBfl_i"
      },
      "outputs": [],
      "source": [
        "# inputs\n",
        "x1 = ...\n",
        "x2 = ...\n",
        "\n",
        "# weights\n",
        "w1 = ...\n",
        "w2 = ...\n",
        "\n",
        "# bias\n",
        "b = ...\n",
        "\n",
        "# output\n",
        "y = ...\n",
        "\n",
        "print(f\"The value of my output neuron, y, before activation = {y}\")\n",
        "\n",
        "# activation function\n",
        "if y < 0:\n",
        "  y = ...\n",
        "\n",
        "# final output\n",
        "print(f\"The value of my output neuron, y, after activation = {y}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J6rsqAUZldOz",
      "metadata": {
        "id": "J6rsqAUZldOz"
      },
      "outputs": [],
      "source": [
        "# Try here with the second set of values!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nFXpdnTDwCMS",
      "metadata": {
        "id": "nFXpdnTDwCMS"
      },
      "source": [
        "# Exercise 2: Two pairs of input neurons\n",
        "\n",
        "\n",
        "Let's consider two pairs of input neurons by repeating what we did in exercise 2 twice, for two different pairs of inputs.  \n",
        "\n",
        "* Define the weights and bias just once! **We want to change the input values only, not the parameters of the neural network**.  \n",
        "* Plot the results! You can use `plt.scatter` to add a circle at the coordinate (or pixel) $x_1, x_2$ colored depending on $y$. Use `plt.colobar()` to see what number each color corresponds to. We use the colormap called `tab10` and provide a minimum and maximum for the colors with `vmin` and `vmax`. In this way we can easily distinguish the values. Check the documentation for `scatter` online for more info!\n",
        "\n",
        "**Question**  \n",
        "What does your image look like with these values:\n",
        "* $ w_1 = 0.2, \\quad w_2 = -0.1, \\quad b = 1$\n",
        "* $x_1 = 1,\\quad x_2 = 3$\n",
        "* $x_1 = 3,\\quad x_2 = 2$  \n",
        "\n",
        "?  \n",
        "Compare with your peers!\n",
        "\n",
        "------\n",
        "📌 Scatter plot  \n",
        "📌 Colormap  \n",
        "📌 Colorbar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BWD0Yly5vy-I",
      "metadata": {
        "id": "BWD0Yly5vy-I"
      },
      "outputs": [],
      "source": [
        "w1 = ...\n",
        "w2 = ...\n",
        "b = ...\n",
        "\n",
        "x1 = ...\n",
        "x2 = ...\n",
        "\n",
        "y = ...\n",
        "\n",
        "if ...:\n",
        "  y = ...\n",
        "\n",
        "print(f\"The value of my output neuron (y) for the first pair of inputs = {y}\")\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(..., ..., c=..., cmap=\"tab10\", vmin=0, vmax=2)\n",
        "\n",
        "x1 = ...\n",
        "x2 = ...\n",
        "\n",
        "y = ...\n",
        "\n",
        "if ...:\n",
        "  y = ...\n",
        "print(f\"The value of my output neuron (y) for the second pair of inputs = {y}\")\n",
        "\n",
        "plt.scatter(..., ..., c=..., cmap=\"tab10\", vmin=0, vmax=2)\n",
        "\n",
        "plt.xlabel(r'$x_1$')\n",
        "plt.ylabel(r'$x_2$')\n",
        "plt.colorbar(label='y')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dXYXQG7nx5Il",
      "metadata": {
        "id": "dXYXQG7nx5Il"
      },
      "source": [
        "# Exercise 3: Two pairs of input neurons, but more sophisticated\n",
        "\n",
        "What's the way to put many different values together in coding languge? **Arrays**!  \n",
        "What's the way to repeat an operation in coding language? **Loops**!\n",
        "\n",
        "Remake of exercise 2:\n",
        "* define the weights and bias as in exercise 2\n",
        "* define an array that containes the two values of $x_1$ you used in exercise 2\n",
        "* same for $x_2$\n",
        "* define an array of two zeros (use `np.zeros`) where you will later store the $y$ outputs: this is called **initialization**! We are setting $y$ to some arbitrary value (zero), and we later compute the right values.\n",
        "* loop through the two pairs of neurons: `for i in range(2):`\n",
        "  * calculate $y$ of the $i$-th input neuron, namely `y[i]`\n",
        "* scatter plot\n",
        "\n",
        "**Hint**  \n",
        "Use `x1[i]` and `x2[i]` to calculate `y[i]`\n",
        "\n",
        "**Question**  \n",
        "Do you get the same results as exercise 3?\n",
        "\n",
        "------\n",
        "📌 Arrays  \n",
        "📌 Loops  \n",
        "📌 Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CwPiNGRGx-IF",
      "metadata": {
        "id": "CwPiNGRGx-IF"
      },
      "outputs": [],
      "source": [
        "w1 = ...\n",
        "w2 = ...\n",
        "b = ...\n",
        "\n",
        "x1 = np.array([..., ...])\n",
        "x2 = np.array([..., ...])\n",
        "\n",
        "y = np.zeros(...)\n",
        "\n",
        "for i in range(...):\n",
        "\n",
        "    # Hint: use the same equations as in the exercises above,\n",
        "    # but don't forget that now we are using arrays so\n",
        "    # we need to access their i-th element in the loop\n",
        "    y[i] = ...\n",
        "\n",
        "    if ...:\n",
        "      y[i] = ...\n",
        "\n",
        "    print(f\"The value of my output neuron for the {i+1}-th pair of inputs = {y[i]}\")\n",
        "\n",
        "plt.scatter(..., ..., c=..., cmap=\"tab10\", vmin=0, vmax=2)\n",
        "\n",
        "plt.xlabel(r'$x_1$')\n",
        "plt.ylabel(r'$x_2$')\n",
        "plt.colorbar(label='y')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CvD7UdtG3Stl",
      "metadata": {
        "id": "CvD7UdtG3Stl"
      },
      "source": [
        "# Exercise 4: Many input neurons\n",
        "\n",
        "Let's generalize!\n",
        "Let's see what our simple neural network gives for many different inputs **samples**.\n",
        "This is like looking at the value of a function $f(x)$ for many different $x$, like you do when graphing $f$ on the Cartesian plane.\n",
        "\n",
        "Generate many input samples, as many as you want (but let's say less than 100, for convenience).\n",
        "\n",
        "* choose the number of pairs of input samples, `N_samples`\n",
        "* define an array with `N_samples` values of $x_1$: you don't want to type by hand so many numbers, so let's use `np.linspace` to automatically generate many, evenly spaced values for $x_1$\n",
        "* same for $x_2$\n",
        "* define an array of size `N_samples` full of zeros (use `np.zeros`) where you will later store the $y$ outputs: again initialization!\n",
        "* loop through the many input pairs: `for i in range(N_samples):`\n",
        "  * calculate $y$ for the $i$-th input pair, namely `y[i]`\n",
        "* scatter plot\n",
        "\n",
        "Use the same weights and bias as in exercises 2 and 3, so that you can check your results. Then feel free to change them!\n",
        "\n",
        "**Clarification**  \n",
        "In this exercise we have many input samples (many samples of $x_1, x_2$), but our neural network only has 2 input neurons and 1 ouput neuron. Don't mistake the concept of neurons with that of samples!\n",
        "\n",
        "**Question**  \n",
        "* What do you obtain? Discuss with your peers and facilitators!  \n",
        "\n",
        "------\n",
        "📌 Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0DTk1jRV4GTW",
      "metadata": {
        "id": "0DTk1jRV4GTW"
      },
      "outputs": [],
      "source": [
        "w1 = ...\n",
        "w2 = ...\n",
        "b = ...\n",
        "\n",
        "N_samples = ...\n",
        "y = np.zeros(...)\n",
        "\n",
        "x1 = np.linspace(-1, 1, ...)\n",
        "x2 = np.linspace(-1, 1, ...)\n",
        "\n",
        "for i in range(...):\n",
        "\n",
        "    y[i] = w1 * x1[...] + ...\n",
        "\n",
        "    if ... :\n",
        "      y[i] = ...\n",
        "\n",
        "plt.scatter(..., ..., c=...)\n",
        "\n",
        "plt.xlabel(r'$x_1$')\n",
        "plt.ylabel(r'$x_2$')\n",
        "plt.colorbar(label='y')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Prj0UKdfZc3O",
      "metadata": {
        "id": "Prj0UKdfZc3O"
      },
      "source": [
        "# Exercise 5: Many neurons in an image\n",
        "\n",
        "Let's make an image!\n",
        "\n",
        "Instead of generating many neurons in sequence, like in exercise 4, let's arrange them in a table, or better, a **matrix**.\n",
        "\n",
        "After all we have two input neurons with values $x_1$ and $x_2$, so we can interpret them as the positions of the **pixels** in an image or as the coordinates in a Cartesian plane (usually denoted as $(x,y)$ instead of $(x_1, x_2)$). Each pixel corresponds to a different input sample.\n",
        "\n",
        "In this way, we can **visualize** 👀 our very simple neural network with a picture 📸.\n",
        "\n",
        "* Define the size of the image by choosing the number of rows and columns $N_{rows}$, $N_{cols}$. Then, the number of input samples will be $N_{samples} = N_{rows} \\times N_{cols}$.\n",
        "\n",
        "* Define the values of the $x_1$ samples as an array of length $N_{cols}$ using `np.linspace`\n",
        "\n",
        "* Define the values of the $x_2$ samples as an array of length $N_{rows}$ using `np.linspace`\n",
        "\n",
        "* Initialize a matrix with $N_{rows}$ rows and $N_{cols}$ columns, full of zeros (use `np.zeros(N_rows, N_cols)`). Here you will later store the $y$ outputs!\n",
        "\n",
        "* Loop through the rows, `for i in range(N_rows):`\n",
        "  * Loop through the columns, `for j in range(N_cols):`\n",
        "    * this is a **nested loop** 🪆!\n",
        "    * calculate $y$ of the $i$-th, $j$-th neuron, namely `y[i,j]`\n",
        "\n",
        "* Make an image of the outputs using `imshow` or `pcolormesh`. [Remember to choose your favorite color map!](https://matplotlib.org/stable/users/explain/colors/colormaps.html) 🎨\n",
        "\n",
        "**Question**  \n",
        "Your image is a visualization of the simple neural network that we defined first in exercise 2. The neural network is defined by the two weights and one bias, and the image shows what the neural outputs for different inputs. What does it look like? Describe the image. Do you think it makes sense?\n",
        "\n",
        "------\n",
        "📌 Matrix  \n",
        "📌 Pixel  \n",
        "📌 Nested loop  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5319v2_sZcm5",
      "metadata": {
        "id": "5319v2_sZcm5"
      },
      "outputs": [],
      "source": [
        "w1 = ...\n",
        "w2 = ...\n",
        "b = ...\n",
        "\n",
        "N_cols = 100\n",
        "N_rows = 200\n",
        "\n",
        "x1 = np.linspace(-1, 1, ...)\n",
        "x2 = np.linspace(-1, 1, ...)\n",
        "\n",
        "y = np.zeros((..., ...))\n",
        "\n",
        "for i in range(...):\n",
        "  for j in range(...):\n",
        "\n",
        "      y[i, j] = w1 * x1[...] + ... * x2[...] + ...\n",
        "      if ...:\n",
        "        y[i, j] = ...\n",
        "\n",
        "plt.pcolormesh(x1, ..., y, cmap=\"...\")\n",
        "\n",
        "# you can also use imshow\n",
        "# plt.imshow(..., cmap=\"...\")\n",
        "\n",
        "plt.xlabel(r'$x_1$')\n",
        "plt.ylabel(r'$x_2$')\n",
        "plt.colorbar(label='y')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PalK_HeI6EWB",
      "metadata": {
        "id": "PalK_HeI6EWB"
      },
      "source": [
        "# **Checkpoint!** 🚩\n",
        "When you reach this point, make sure your results are correct ✅.  \n",
        "Check in with the facilitators or your colleagues.  \n",
        "Compare your images with the ones in the slides.  \n",
        "Ask questions 🙋!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JB0XohhBdIpC",
      "metadata": {
        "id": "JB0XohhBdIpC"
      },
      "source": [
        "# Exercise 6: Let's abstract and generalize\n",
        "\n",
        "Here we make the code more general so that it can be reused to add multiple hidden layers so that we can visualize much more complex neural networks.\n",
        "\n",
        "Because the code will get more and more complicated, we have written most of it for you. You only have to fill out some crucial details. Follow the instructions!\n",
        "\n",
        "So, now we use:\n",
        "* a matrix to store the various output values\n",
        "* another matrix to store the weights\n",
        "* an array for the biases\n",
        "\n",
        "This may seem reduntant for now, but you'll see how it will become convenient in the next few exercises.  \n",
        "\n",
        "We re-write exercise 5, taking into account these additional considerations:\n",
        "\n",
        "* define the number of neurons in the input layer, $N_{in}$ (not the samples!)\n",
        "* define the number of neurons in the output layer, $N_{out}$\n",
        "* store the weights in a matrix of size $N_{out}, N_{in}$\n",
        "* define the biases in an array of size $N_{out}$\n",
        "* define a function named `apply_layer` that takes the inputs from the previous layer, the weights and the biases, and provides the value of the output neuron $y$: this is **feedforward**! ⏭️\n",
        "* call the function `apply_layer` in the nested loop to apply the feedforward to all input samples\n",
        "* make the image\n",
        "\n",
        "__Fill up the blanks and make the code work!__\n",
        "\n",
        "**Questions**\n",
        "* Do you recognize the operations inside `apply_layer`? The function has changed, but it essentially performing the same operations as in the previous exercise.\n",
        "\n",
        "**Some considerations**  \n",
        "Technical, tiny, specific details are unavoidable in life...especially when dealing with code development, but not only (have you ever tried reading an electric bill?) 😮‍💨.\n",
        "That's why we had to modify the code and now it looks a bit uglier 🙈.\n",
        "Sorry about that.\n",
        "The main technicalities are here:\n",
        "* `np.dot` is doing a multiplication that works also if the arrays are multidimensional\n",
        "* `np.where` is applying the activation function in a numpy-friendly way\n",
        "* the `if` condition in `apply_layer` captures the case where the output `y` is single number\n",
        "\n",
        "------\n",
        "📌 Feedforward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sk21J1lB8Ixt",
      "metadata": {
        "id": "sk21J1lB8Ixt"
      },
      "outputs": [],
      "source": [
        "w1 = 0.2\n",
        "w2 = -0.1\n",
        "b = 1\n",
        "\n",
        "# size of the image\n",
        "N_cols = 200\n",
        "N_rows = 200\n",
        "\n",
        "# outputs\n",
        "y = np.zeros((N_rows, N_cols))\n",
        "\n",
        "# input samples\n",
        "x1 = np.linspace(-1, 1, N_cols)\n",
        "x2 = np.linspace(-1, 1, N_rows)\n",
        "\n",
        "N_in = ... # input layer size\n",
        "N_out = ... # output layer size\n",
        "\n",
        "# weights: matrix with N_out rows and N_in columns\n",
        "weights = np.zeros((N_out,...))\n",
        "weights[0,0] = w1\n",
        "weights[0,1] = w2\n",
        "\n",
        "# biases: array of size N_out\n",
        "biases = np.zeros(...)\n",
        "biases[0] = b\n",
        "\n",
        "# feedforward from one layer to the next\n",
        "# calculate the output given the inputs, weights, biases\n",
        "def apply_layer(x, w, b):\n",
        "    y = np.dot(w, x) + b\n",
        "    y = np.where(y>0, y, 0.)\n",
        "    if y.size == 1:\n",
        "      return y[0]\n",
        "    else:\n",
        "      return y\n",
        "\n",
        "# loop through all the pixels / input samples\n",
        "for i in range(...):\n",
        "  for j in range(...):\n",
        "\n",
        "      inputs = np.array([x1[j], x2[i]])\n",
        "\n",
        "      # pass to the apply_layer function the correct inputs\n",
        "      # Hint: x represents the inputs, w the weights and b the biases\n",
        "      y[i, j] = apply_layer(..., ..., ...)\n",
        "\n",
        "plt.pcolormesh(x1, x2, y, cmap=\"rainbow\")\n",
        "plt.xlabel(r'$x_1$')\n",
        "plt.ylabel(r'$x_2$')\n",
        "plt.colorbar(label='y')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xQNeasI3_5h-",
      "metadata": {
        "id": "xQNeasI3_5h-"
      },
      "source": [
        "# Exercise 7: One hidden layer with random weights and biases\n",
        "\n",
        "We now add another layer of complexity, literally 😉!  \n",
        "We add a hidden layer with $N_{hidden}$ neurons and expand upon exercise 6.  \n",
        "\n",
        "**Don't panic** if you don't understand every single line of code, but **make sure you get the underlying logic**! 🧠\n",
        "\n",
        "**One hidden layer means that we need the following:**\n",
        "* weights from the input layer to the hidden layer\n",
        "* biases of the hidden layer\n",
        "* feedforward: input layer ➡️ first hidden layer\n",
        "* weights from the hidden layer to the output layer\n",
        "* bias(es) of the output layer\n",
        "* feedforward: hidden layer ➡️ output layer\n",
        "\n",
        "**Here is what we do in this exercise in addition to what we already did in exercise 5 and 6:**\n",
        "\n",
        "* Choose the number of neurons in the hidden layer $N_{hidden}$ (>1)\n",
        "\n",
        "* Define the weights from the input layer to the hidden layer in a matrix of size $N_{hidden} \\times N_{in}$. Choose the weights randomly, to make it funkier and to avoid having to define each one of them by hand.\n",
        "\n",
        "* Define the biases of the neurons in the hidden layer and store them in an array of size $N_{hidden}$. Choose them randomly.\n",
        "\n",
        "* Define the weights and biases of the connections from the first hidden layer and the second hidden layer\n",
        "\n",
        "* Define the weights from the hidden layer to the output layer in a random matrix of size $N_{out} \\times N_{hidden}$\n",
        "\n",
        "* Define the biases of the output layer in a random array of size $N_{out}$.\n",
        "\n",
        "* Use the function `apply_layer` twice: first to move from the input to the hidden layer, then from the hidden to the output layer\n",
        "\n",
        "Note that to have random weights and biases we use `np.random.uniform`.\n",
        "\n",
        "__Fill up the blanks, choose the parameters, and make the code work!__\n",
        "\n",
        "**Question**  \n",
        "* What do you observe if you progressively increase the number of neurons in the hidden layer?\n",
        "\n",
        "* Play around with the minimum and maximum weight and bias. What happens if the biases are much larger than the weights? How about the other way around?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4EM4B5v0_9Xd",
      "metadata": {
        "id": "4EM4B5v0_9Xd"
      },
      "outputs": [],
      "source": [
        "N_in = ... # number of neurons in the input layer\n",
        "N_out = ... # number of neurons in the output layer\n",
        "N_hidden = ... # number of neurons in the hidden layer - you choose this number!\n",
        "\n",
        "w_max = ... # maximum weigth - you choose this number!\n",
        "b_max = ... # maximum bias - you choose this number!\n",
        "\n",
        "# connections from the input layer and the hidden layer\n",
        "weights_1 = np.random.uniform(low=-w_max, high=w_max, size=(N_hidden,N_in)) # weights\n",
        "biases_1 = np.random.uniform(low=-b_max, high=b_max, size=N_hidden) # biases\n",
        "\n",
        "# connections from the hidden layer to the output layer\n",
        "weights_2 = np.random.uniform(low=-w_max, high=w_max, size=(N_out,N_hidden)) # weights\n",
        "biases_2 = np.random.uniform(low=-b_max, high=b_max, size=N_out) # biases\n",
        "\n",
        "# size of the image\n",
        "N_cols = 100\n",
        "N_rows = 100\n",
        "\n",
        "# outputs\n",
        "y = np.zeros((N_rows, N_cols))\n",
        "\n",
        "# input samples\n",
        "x1 = np.linspace(-1, 1, N_cols)\n",
        "x2 = np.linspace(-1, 1, N_rows)\n",
        "\n",
        "# feedforward from one layer to the next\n",
        "# calculate the output given the inputs, weights, biases\n",
        "def apply_layer(inputs, weights, biases):\n",
        "    y = np.dot(weights, inputs) + biases\n",
        "    y = np.where(y>0, y, 0.)\n",
        "    if y.size == 1:\n",
        "      return y[0]\n",
        "    else:\n",
        "      return y\n",
        "\n",
        "# feedforward through the whole neural network\n",
        "# loop through all the pixels / input samples\n",
        "for i in range(N_rows):\n",
        "  for j in range(N_cols):\n",
        "      inputs = np.array([x1[j], x2[i]])\n",
        "      # feedforward: input layer ➡️ hidden layer\n",
        "      y_hidden = apply_layer(inputs, weights_1, biases_1)\n",
        "      # feedforward: hidden layer ➡️ output layer\n",
        "      y[i,j] = apply_layer(y_hidden, weights_2, biases_2)\n",
        "\n",
        "plt.pcolormesh(x1, x2, y, cmap=\"rainbow\")\n",
        "\n",
        "plt.xlabel(r'$x_1$')\n",
        "plt.ylabel(r'$x_2$')\n",
        "plt.colorbar(label='y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YbZpO1SHElFj",
      "metadata": {
        "id": "YbZpO1SHElFj"
      },
      "source": [
        "# Exercise 8: Two hidden layers\n",
        "\n",
        "Let's add yet another layer of complexity 🤯.  \n",
        "Here we do the same as in exercise 7, but with an additional hidden layer.  \n",
        "\n",
        "This means that we need the weights between the first and second hidden layer and the biases of the second hidden layer.  \n",
        "Then, we need to feedforward not once, not twice, but thrice!\n",
        "\n",
        "1. input layer ➡️ first hidden layer\n",
        "2. first hidden layer ➡️ second hidden layer\n",
        "3. second hidden layer ➡️ output layer\n",
        "\n",
        "Note how the inputs of one feedforward, are the ouputs of the previous feeedforward.\n",
        "\n",
        "__Fill up the blanks, choose the parameters, and make the code work!__\n",
        "\n",
        "**Questions**\n",
        "* What do you observe when changing the number of neurons in the first and second hidden layer?\n",
        "\n",
        "* What changes do you observe in the image from exercise 7?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4-f5eknLFMDH",
      "metadata": {
        "id": "4-f5eknLFMDH"
      },
      "outputs": [],
      "source": [
        "N_in = 2 # number of neurons in the input layer\n",
        "N_out = 1 # number of neurons in the output layer\n",
        "N_hidden1 = ... # number of neurons in the first hidden layer - you choose this number!\n",
        "N_hidden2 = ... # number of neurons in the second hidden layer - you choose this number!\n",
        "\n",
        "w_max = ... # maximum weigth - you choose this number!\n",
        "b_max = ... # maximum bias - you choose this number!\n",
        "\n",
        "# connections from the input layer and the first hidden layer\n",
        "weights_1 = np.random.uniform(low=-w_max, high=w_max, size=(N_hidden1,N_in)) # weights\n",
        "biases_1 = np.random.uniform(low=-b_max, high=b_max, size=N_hidden1) # biases\n",
        "\n",
        "# connections from the first hidden layer to the second hidden layer\n",
        "weights_2 = np.random.uniform(low=-w_max, high=w_max, size=(N_hidden2,N_hidden1)) # weights\n",
        "biases_2 = np.random.uniform(low=-b_max, high=b_max, size=N_hidden2) # biases\n",
        "\n",
        "# connections from the second hidden layer to the output layer\n",
        "weights_3 = np.random.uniform(low=-w_max, high=w_max, size=(N_out,N_hidden2)) # weights\n",
        "biases_3 = np.random.uniform(low=-b_max, high=b_max, size=N_out) # biases\n",
        "\n",
        "# size of the image\n",
        "N_cols = 100\n",
        "N_rows = 100\n",
        "\n",
        "# outputs\n",
        "y = np.zeros((N_rows, N_cols))\n",
        "\n",
        "# input samples\n",
        "x1 = np.linspace(-1, 1, N_cols)\n",
        "x2 = np.linspace(-1, 1, N_rows)\n",
        "\n",
        "# feedforward from one layer to the next\n",
        "# calculate the output given the inputs, weights, biases\n",
        "def apply_layer(inputs, weights, biases):\n",
        "    y = np.dot(weights, inputs) + biases\n",
        "    y = np.where(y>0, y, 0.)\n",
        "    if y.size == 1:\n",
        "      return y[0]\n",
        "    else:\n",
        "      return y\n",
        "\n",
        "# feedforward through the whole neural network\n",
        "# loop through all the pixels / input samples\n",
        "for i in range(N_rows):\n",
        "  for j in range(N_cols):\n",
        "      inputs = np.array([x1[j], x2[i]])\n",
        "\n",
        "      # input layer ➡️ 1st hidden layer\n",
        "      y_hidden1 = apply_layer(inputs, weights_1, biases_1)\n",
        "\n",
        "      # 1st hidden layer ➡️ 2nd hidden layer\n",
        "      # Hint: you need the values of the neurons in the 1st hidden layer\n",
        "      # (y_hidden1) to move forward\n",
        "      y_hidden2 = apply_layer(..., weights_2, biases_2)\n",
        "\n",
        "      # 2nd hidden layer ➡️ output layer\n",
        "      # Hint: you need the values of the neurons in the 2nd hidden layer\n",
        "      # (y_hidden2) to move forward\n",
        "      y[i, j] = apply_layer(..., weights_3, biases_3)\n",
        "\n",
        "plt.pcolormesh(x1, x2, y, cmap=\"rainbow\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2qjGzm1DEoeU",
      "metadata": {
        "id": "2qjGzm1DEoeU"
      },
      "source": [
        "# Exercise 9: Many hidden layers\n",
        "\n",
        "The following code is a generalization of exercise 8 for an arbitrary number of hidden layers 😱.   \n",
        "\n",
        "To make the calculations more efficient and compact, we re-wrote part of the code and took care of a few other technicalities. Nonetheless, you should still be able to understand what the code does. **Follow the logic**, do not get lost in the details! 🤓\n",
        "\n",
        "**Now, it's time to really play with your parameters.**\n",
        "\n",
        "* Choose the parameters of the neural network:\n",
        "  * number of hidden layers: `N_hidden_layers`\n",
        "  * number of neurons in each hidden layer: `N_hidden_neurons`\n",
        "  * maximum weight and bias: `w_max, b_max`\n",
        "\n",
        "* Make a higher-resolution image, for example of size 400 x 400\n",
        "* [Choose your favorite color map!](https://matplotlib.org/stable/users/explain/colors/colormaps.html) 🎨\n",
        "\n",
        "* Try three different activation functions:\n",
        "  * the one that we have been using so far called ReLU: `np.where(y>0, y, 0.)`\n",
        "  * sigmoid: `(1/(1+np.exp(-y)))`, it's a smooth step from 0 to 1\n",
        "  * `np.where(y>0, 1., 0.)`, a sharp step from 0 to 1\n",
        "\n",
        "**The image is automatically saved in your drive a png file named `nn.png`.**  \n",
        "**Play around, pick the coolest image you get, download it, and upload it to Google Classroom!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RlvehmeGgzNs",
      "metadata": {
        "id": "RlvehmeGgzNs"
      },
      "outputs": [],
      "source": [
        "N_in = 2 # number of neurons in the input layer\n",
        "N_out = 1 # number of neurons in the output layer\n",
        "\n",
        "# number of hidden layers\n",
        "N_hidden_layers = ...\n",
        "# number of neurons in every hidden layer (all hidden layers have the same number of hidden neurons)\n",
        "N_hidden_neurons = ...\n",
        "\n",
        "# max weight & max bias of the hidden layers\n",
        "w_max = ...\n",
        "b_max = ...\n",
        "\n",
        "# weights and biases throughout the hidden layers\n",
        "weights = np.random.uniform( low = -w_max, high = w_max, size = [N_hidden_layers, N_hidden_neurons, N_hidden_neurons] )\n",
        "biases  = np.random.uniform( low = -b_max, high = b_max, size = [N_hidden_layers, N_hidden_neurons] )\n",
        "\n",
        "# from the input layer to the first hidden layer\n",
        "weights_first = np.random.uniform( low = -1, high = 1, size = [N_in, N_hidden_neurons] )\n",
        "biases_first  = np.random.uniform( low = -1, high = 1, size = N_hidden_neurons )\n",
        "\n",
        "# from the last hidden layer layer to the output layer\n",
        "weights_last = np.random.uniform( low = -1, high = 1, size = [N_hidden_neurons, N_out] )\n",
        "biases_last  = np.random.uniform( low = -1, high = 1, size = N_out )\n",
        "\n",
        "# move from one layer to the next one\n",
        "def apply_layer(inputs, weights, biases):\n",
        "    y = np.dot(inputs, weights) + biases\n",
        "    # activation function\n",
        "    y = np.where(y>0, y, 0.)\n",
        "    #y = 1 / ( 1+np.exp(-y) ) # Try this other activation function! The sigmoid\n",
        "    return y\n",
        "\n",
        "# function that evaluates the whole NN\n",
        "def apply_network(inputs):\n",
        "    # input layer ➡️ first hidden layer\n",
        "    y = apply_layer( inputs , weights_first, biases_first)\n",
        "    # move through all the hidden layers\n",
        "    for j in range(N_hidden_layers):\n",
        "        y = apply_layer( y, weights[j,:,:], biases[j,:] )\n",
        "    # last hidden layer ➡️ output layer\n",
        "    output = apply_layer( y, weights_last, biases_last )\n",
        "    return(output)\n",
        "\n",
        "# Size of the image - make it high res!\n",
        "N_cols = ...\n",
        "N_rows = ...\n",
        "\n",
        "# Generate all the possible input values x1, x2\n",
        "# This is done in a clever way to make calculations faster\n",
        "# Clever here means using arrays instead of matrices, for speed\n",
        "x1, x2 = np.meshgrid(np.linspace(-1, 1, N_cols), np.linspace(-1, 1, N_rows))\n",
        "N_pixels = N_rows * N_cols\n",
        "inputs = np.zeros([N_pixels,2])\n",
        "inputs[:,0] = x1.flatten()\n",
        "inputs[:,1] = x2.flatten()\n",
        "\n",
        "# Apply the NN to all the input values at once!\n",
        "outputs = apply_network(inputs)\n",
        "# Re-arrange in a 2D image\n",
        "outputs = np.reshape(outputs[:,0],[N_rows, N_cols])\n",
        "\n",
        "# Plot it! And make it fancy!\n",
        "plt.figure(figsize=[10,10], dpi=300)\n",
        "plt.axes([0,0,1,1])\n",
        "plt.axis('off')\n",
        "plt.imshow(output, cmap=\"...\")\n",
        "\n",
        "# Save the figure in your Drive\n",
        "plt.savefig('nn.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SqI-Jn2zMMGZ",
      "metadata": {
        "id": "SqI-Jn2zMMGZ"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Download your image and upload it to Google Classroom.  \n",
        "We will print them out and make an art wall with them!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Acknowledgements:\n",
        "Alisa Bettale, Sage Miller, Evan Neill, Florian Florian Marquardt*\n"
      ],
      "metadata": {
        "id": "-m3Ye9Oso7rt"
      },
      "id": "-m3Ye9Oso7rt"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}